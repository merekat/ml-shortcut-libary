{
    // IMPORT LIBARIES
    "Import Basic Libaries": {
        "prefix": "import_basics",
        "body": [
            "# Import of basic packages",
            "import numpy as np",
            "import pandas as pd",
            "import operator, datetime, time, pathlib, shutil, tempfile, sys, warnings",
            "from joblib import dump, load",
            "",
            "warnings.filterwarnings('ignore')",
            "",
            "# Set random seed ",
            "RSEED = 0"
        ],
        "description": "Import Basic Libaries"
    },
    "Import Chart Libaries": {
        "prefix": "import_charts",
        "body": [
            "# Import of chart packages",
            "import matplotlib.pyplot as plt",
            "import seaborn as sns",
            "import plotly.express as px",
            "import altair as alt"
        ],
        "description": "Import Chart Libaries"
    },
    "Import Preprossesor Libaries": {
        "prefix": "import_preprossesor",
        "body": [
            "# Import of preprossesor packages",
            "from sklearn import preprocessing",
            "from sklearn.impute import SimpleImputer",
            "from sklearn.compose import ColumnTransformer",
            "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelBinarizer, PolynomialFeatures",
            "import tensorflow as tf",
            "from tensorflow import keras",
            "from tensorflow.keras import layers",
            "from tensorflow.keras.layers.experimental import preprocessing",
            "from tensorflow.keras.utils import to_categorical",
            "from tensorflow.keras.preprocessing import image"
        ],
        "description": "Import preprossesor Libaries"
    },
    "Import Time Series Libaries": {
        "prefix": "import_timeseries",
        "body": [
            "# Import of time series packages",
            "import pmdarima as pm",
            "from statsmodels.tsa.arima_model import ARIMA",
            "from prophet import Prophet",
            "import adfuller.formula.api as smf",
            "from statsmodels.tsa.seasonal import seasonal_decompose",
            "from scipy.ndimage import gaussian_filter",
            "from calendar import monthrange",
            "from calendar import month_name"
        ],
        "description": "Import Time Series Libaries"
    },
    "Import ML Regressor Libaries": {
        "prefix": "import_ml_regressor",
        "body": [
            "# Import of machine learning regressor packages",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV, cross_val_predict, RandomizedSearchCV",
            "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression",
            "from sklearn.neighbors import KNeighborsRegressor",
            "from sklearn.tree import DecisionTreeRegressor, plot_tree",
            "from sklearn.ensemble import RandomForestRegressor, StackingRegressor",
            "from sklearn.svm import SVC"
        ],
        "description": "Import Regressor Libaries"
    },
    "Import ML Classifier Libaries": {
        "prefix": "import_ml_classification",
        "body": [
            "# Import of machine learning classifier packages",
            "from sklearn.pipeline import Pipeline",
            "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_predict, RandomizedSearchCV",
            "from sklearn.linear_model import SGDClassifier",
            "from sklearn.neighbors import  KNeighborsClassifier",
            "from sklearn.tree import DecisionTreeClassifier, plot_tree",
            "from sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, RandomForestClassifier, VotingClassifier, StackingClassifier, AdaBoostClassifier",
            "from xgboost import XGBClassifier",
            "from sklearn.svm import SVC"
        ],
        "description": "Import Classifier Libaries"
    },
    "Import ML FNN Libaries": {
        "prefix": "import_ml_fnn",
        "body": [
            "# Import of FNN packages",
            "import keras",
            "from keras.models import Sequential",
            "from keras.layers import Dense, Activation, Dropout",
            "from tensorflow.keras import layers, regularizers"
        ],
        "description": "Import FNN Libaries"
    },
    "Import ML CNN Libaries": {
        "prefix": "import_ml_cnn",
        "body": [
            "# Import of CNN packages",
            "import tensorflow as tf",
            "from tensorflow.keras.models import Sequential",
            "from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Activation, Flatten, Dropout, GlobalAveragePooling2D",
            "from tensorflow.keras.applications.${1:model} import decode_predictions, preprocess_input",
            "from tensorflow.keras.optimizers import Adam",
            "from tensorflow.keras import Input, Model"
        ],
        "description": "Import CNN Libaries"
    },
    "Import ML NLP Libaries": {
        "prefix": "import_ml_nlp",
        "body": [
            "# Import of NLP packages",
            "import nltk",
            "from transformers import pipeline",
            "from nltk.stem import PorterStemmer",
            "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer",
            "from tqdm import tqdm"
        ],
        "description": "Import NLP Libaries"
    },
    "Import Metrics Libaries": {
        "prefix": "import_metrics",
        "body": [
            "# Import of machine learning metric packages",
            "from sklearn.metrics import make_scorer, f1_score, classification_report, confusion_matrix, mean_squared_error, r2_score, accuracy_score, recall_score, precision_score, roc_auc_score, roc_curve, fbeta_score",
            "from sklearn import metrics",
            "from scipy.stats import randint, uniform",
            "from statsmodels.tsa.stattools import adfuller, kpss"
        ],
        "description": "Import of Metrics Libaries"
    },
    "Import Database Libaries": {
        "prefix": "import_dabase",
        "body": [
            "# Import of database packages",
            "import os",
            "from dotenv import load_dotenv",
            "from sqlalchemy import create_engine"
        ],
        "description": "Import of Database Libaries"
    },
    // RANDOM
    "Random DF": {
        "prefix": "df_random",
        "body": [
            "np.random.seed(RSEED)",
            "",
            "# Create a DataFrame with ${1:rows} rows and 3 columns of random numbers",
            "df = pd.DataFrame({",
            "    'Col1': np.random.rand(${1:rows}),",
            "    'Col2': np.random.randint(0, 2, size=${1:rows}),",
            "    'Col3': np.random.normal(0, 1, size=${1:rows})",
            "})"
        ],
        "description": "Random DF"
    },
    // SET STYLES
    "Styles": {
        "prefix": "style",
        "body": [
            "# Set styles",
            "plt.style.use('https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-light.mplstyle')",
            "plt.rcParams['figure.figsize'] = (7,4)",
            "plt.style.use('fivethirtyeight')",
            "plt.rcParams['font.size'] = 18",
            "np.set_printoptions(precision=3, suppress=True)"
        ],
        "description": "Set Styles"
    },
    // LOAD & SAVE
    "Load Database": {
        "prefix": "load_db",
        "body": [
            "# Connect to database",
            "load_dotenv()",
            "DB_STRING = os.getenv('DB_STRING')",
            "db = create_engine(DB_STRING)",
            "",
            "# Import of query",
            "query = \"set SCHEMA '${1:schema}'; SELECT * FROM ${2:dataset1} ${3:shortcut1} LEFT JOIN ${4:dataset2} ${5:shortcut2} ON $3.${6:column1} = $5.${7:column2}\"",
            "",
            "# Store as DataFrame",
            "df = pd.read_sql(query, db)",
            "df.head(3)"
        ],
        "description": "Load Data from Databse"
    },
    "Load File": {
        "prefix": "load_file",
        "body": [
            "# Load ${2:filetype}",
            "df = pd.read_${2:filetype}('${1:path/name}.${2:filetype}')",
            "df.head(3)"
        ],
        "description": "Load File"
    },
    "Load Image": {
        "prefix": "load_img",
        "body": [
            "# Load image",
            "img = image.load_img('${1:path/name}.${2:filetype}',target_size=(224,224)) "
        ],
        "description": "Load Image"
    },
    "Load Model": {
        "prefix": "load_model",
        "body": [
            "# Load model",
            "model = load('${1:path/name}.joblib')"
        ],
        "description": "Load File"
    },
    "Save File": {
        "prefix": "save_file",
        "body": [
            "# Save ${2:filetype}",
            "df.to_${2:filetype}('${1:path/name}.${2:filetype}')"
        ],
        "description": "Save File"
    },
    "Save Log": {
        "prefix": "save_log",
        "body": [
            "# Clear logs from previous runs",
            "!rm -rf my_logs/",
            "",
            "# Define path for new directory ",
            "root_logdir = os.path.join(os.curdir, 'logs')",
            "",
            "# Define function for creating a new folder for each run",
            "def get_run_logdir():",
            "    run_id = time.strftime('run_%d_%m_%Y-%H_%M_%S')",
            "    return os.path.join(root_logdir, run_id)",
            "run_logdir = get_run_logdir()",
            "",
            "# Create function for using callbacks",
            "def callbacks(${1:model}):",
            "    return tf.keras.callbacks.TensorBoard(run_logdir+name, histogram_freq=1)"
        ],
        "description": "Save Model"
    },
    "Save Model Continuously": {
        "prefix": "save_model_steps",
        "body": [
            "# Define path where checkpoints should be stored",
            "checkpoint_path = 'models/cp.ckpt'",
            "checkpoint_dir = os.path.dirname(checkpoint_path)",
            "",
            "# Create a callback that saves the model's weights",
            "callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=0)"
        ],
        "description": "Save Model Continuously"
    },
    "Save Model": {
        "prefix": "save_model",
        "body": [
            "# Save the model",
            "save(model, '${1:path/name}.joblib')"
        ],
        "description": "Save Model"
    },
    "Export Model": {
        "prefix": "export_model",
        "body": [
            "# Export the model",
            "dump(model, '${1:path/name}.joblib')"
        ],
        "description": "Save Model"
    },
    // EXPLORATIVE DATA ANALYSIS
    "EDA Shape": {
        "prefix": "eda_shape",
        "body": [
            "# EDA shape",
            "print('Number of rows and columns: ',df.shape)",
            "print('-'*50)",
            "pd.concat([df.head(3), df.tail(3)]).reset_index(drop=True)"
        ],
        "description": "EDA Shape"
    },
    "EDA Info & Describe": {
        "prefix": "eda_info",
        "body": [
            "# EDA info & describe",
            "info = pd.concat([",
            "df.dtypes.to_frame().T,",
            "df.mean(numeric_only=True).to_frame().T,",
            "df.std(numeric_only=True).to_frame().T,",
            "df.min(numeric_only=True).to_frame().T,",
            "df.quantile(0.25, numeric_only=True).to_frame().T,",
            "df.quantile(0.5, numeric_only=True).to_frame().T, ",
            "df.quantile(0.75, numeric_only=True).to_frame().T,",
            "df.max(numeric_only=True).to_frame().T,], ignore_index=True).applymap(lambda x: round(x, 1) if isinstance(x, (int, float)) else x)",
            "",
            "info.insert(0, 'statistic', ['dtype', 'mean', 'std', 'min', '25%', '50%', '75%', 'max'])",
            "info"
        ],
        "description": "EDA Info & Describe"
    },
    "EDA NaNs": {
        "prefix": "eda_nan",
        "body": [
            "# EDA NaNs",
            "missing = pd.DataFrame(df.isnull().sum(), columns=['Number'])",
            "missing['Percentage'] = round((missing['Number']/df.shape[0]) * 100, 1)",
            "missing[missing['Number'] != 0]"
        ],
        "description": "EDA NaNs"
    },
    "EDA Duplicates": {
        "prefix": "eda_dup",
        "body": [
            "# EDA duplicates",
            "duplicates = df.duplicated().sum()",
            "duplicate_percentage = round((duplicates / df.shape[0]) * 100, 1)",
            "df[df.duplicated(keep=False)].head(10)"
        ],
        "description": "EDA Duplicates"
    },
    "EDA Uniques": {
        "prefix": "eda_uni",
        "body": [
            "# EDA Uniques",
            "unique_counts = pd.DataFrame(df.nunique(), columns=['Number']).sort_values('Number', ascending=False).T",
            "unique_counts"
        ],
        "description": "EDA Uniques"
    },
    "Show Image": {
        "prefix": "image_show",
        "body": [
            "# Plot image",
            "plt.imshow(img)"
        ],
        "description": "Show Image"
    },
    // CLEANING
    "Clean Column Names": {
        "prefix": "clean_column",
        "body": [
            "# Clean column names",
            "df.columns = df.columns.str.replace(' ','_')",
            "df.columns = df.columns.str.lower()"
        ],
        "description": "Clean column names"
    },
    "Clean Column Values": {
        "prefix": "clean_value",
        "body": [
            "# Clean column values",
            "df['${1:column}'] = df.${1:column}.str.strip('${2:unit}').astype('${3:type}')"
        ],
        "description": "Clean column values"
    },
    "Fill NaNs with Mean": {
        "prefix": "clean_value",
        "body": [
            "# Fill NaNs with mean of column",
            "y_train.fillna(y_train.mean())",
            "y_test.fillna(y_train.mean())"
        ],
        "description": "Fill NaNs with Mean"
    },
    // ENCODING
    "OrdinalEncoder": {
        "prefix": "encoder_ordinal",
        "body": [
            "# Transform categorical column with natural order in integer",
            "oe = OrdinalEncoder(categories=[['${2:value}', '${3:value}', '${4:value}']])",
            "encoded = oe.fit_transform(df[['${1:column}']])",
            "df_encoded = pd.DataFrame(encoded, columns=lb.classes_, index=df.index)",
            "df = pd.concat([df.drop('${1:column}', axis=1), df_encoded], axis=1)"
        ],
        "description": "OrdinalEncoder"
    },
    "Label Binarizer": {
        "prefix": "encoder_binarizer",
        "body": [
            "# Transform categorical column without natural order in integer",
            "lb = LabelBinarizer()",
            "encoded = lb.fit_transform(df['${1:column}'])",
            "df_encoded = pd.DataFrame(encoded, columns=lb.classes_, index=df.index)",
            "df = pd.concat([df.drop('${1:column}', axis=1), df_encoded], axis=1)"
        ],
        "description": "Label Binarizer"
    },
    "One-Hot Encoder": {
        "prefix": "encoder_one-hot",
        "body": [
            "# Transform categorical column with one-hot-encoding",
            "to_categorical(ytest)"
        ],
        "description": "One-Hot Encoder"
    },
    // LOG TRANSFORM
    "Logarithmic Transformation": {
        "prefix": "transform_log",
        "body": [
            "# Log-transform skewed feature",
            "X['${1:feature}'] = X['${1:feature}'].apply(lambda x: np.log(x + 1))"
        ],
        "description": "Logarithmic Transformation"
    },
    // OUTLIER REDUCTION
    "RobustScaler": {
        "prefix": "scaler_robust",
        "body": [
            "# Reduce outlier influence of the features",
            "scaler = RobustScaler()",
            "X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)"
        ],
        "description": "RobustScaler"
    },
    // NORMALIZATION
    "MinMaxScaler": {
        "prefix": "scaler_minmax",
        "body": [
            "# Normalize the features",
            "scaler = MinMaxScaler()",
            "X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)"
        ],
        "description": "MinMaxScaler"
    },
    "MaxAbsScaler": {
        "prefix": "scaler_maxabs",
        "body": [
            "# Normalize the features",
            "scaler = MaxAbsScaler()",
            "X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)"
        ],
        "description": "MaxAbsScaler"
    },
    "NormScaler": {
        "prefix": "scaler_normalization",
        "body": [
            "# Normalize the features",
            "scaler = preprocessing.Normalization()",
            "# OR",
            "scaler = tf.keras.layers.Normalization(axis=-1)",
            "scaler.adapt(X_train)"
        ],
        "description": "NormScaler"
    },
    // STANDARDIZATION
    "StandardScaler": {
        "prefix": "scaler_standard",
        "body": [
            "# Standarize the features",
            "scaler = StandardScaler()",
            "X = pd.DataFrame(scaler.fit_transform(X), index=X.index, columns=X.columns)"
        ],
        "description": "StandardScaler"
    },
    // IMAGE ENCODING
    "Image to Array": {
        "prefix": "image_array",
        "body": [
            "# Convert image to array",
            "img = image.img_to_array(${1:image}, dtype='uint8')"
        ],
        "description": "Image to Array"
    },
    "Reshape Image": {
        "prefix": "image_reshape",
        "body": [
            "# Reshape to match the input shape required by the model",
            "img = img.reshape(1, 224, 224, 3)"
        ],
        "description": "Reshape Image"
    },
    "Preprocess Image": {
        "prefix": "image_preprocess",
        "body": [
            "# Preprocessing images",
            "img = preprocess_input(${1:image})"
        ],
        "description": "Preprocess Image"
    },
    // PLOTTING
    "Histogram Matplot": {
        "prefix": "histogram_plt",
        "body": [
            "# Create the histogram",
            "plt.figure(figsize=(10, 6))",
            "",
            "plt.hist(df['${1:value}'], bins=30, label='${1:value}')",
            "",
            "# Customize the plot",
            "plt.title('Histogram ${1:value}')",
            "plt.xlabel('${1:value}')",
            "plt.ylabel('Frequency')",
            "plt.legend()",
            "plt.show()"
        ],
        "description": "Histogram Matplot"
    },
    "Pairplot Seaborn": {
        "prefix": "pairplot_sns",
        "body": [
            "# Create Pairplot",
            "sns.set(style='ticks')",
            "plt.figure(figsize=(20, 20))",
            "pairplot = sns.pairplot(df, diag_kind='kde', plot_kws={'alpha': 0.6, 's': 30, 'edgecolor': 'k'}, height=2)",
            "",
            "plt.tight_layout()",
            "plt.show()"
        ],
        "description": "Pairplot Seaborn"
    },
    "Confusion Matrix Matplot": {
        "prefix": "confusion_matrix_plt",
        "body": [
            "# Create confusion matrix",
            "conf_matrixf = pd.DataFrame(conf_matrix)",
            "",
            "# Plot confusion matrix",
            "plt.figure(figsize=(10,7))",
            "sns.heatmap(conf_matrixf, annot=True, fmt='d', cmap='YlGnBu')",
            "plt.xlabel('Predicted')",
            "plt.ylabel('True')",
            "plt.title('Confusion Matrix')",
            "plt.show()"
        ],
        "description": "Confusion Matrix"
    },
    "Correlation Heatmap Matplot": {
        "prefix": "heatmap_correlation_plt",
        "body": [
            "# Create correlation heatmap",
            "correlations = df.corr()",
            "mask = np.triu(np.ones_like(correlations, dtype=bool))",
            "",
            "plt.figure(figsize=(10,8))",
            "sns.heatmap(correlations, vmax=1, vmin=-1, annot=True, mask=mask, cmap='YlGnBu')",
            "plt.title('Correlation Heatmap')",
            "plt.show()"
        ],
        "description": "Correlation Heatmap"
    },
    "Multi Bar Plot Matplot": {
        "prefix": "bar_multi_plt",
        "body": [
            "#Plot ${1:xValue} against ${2:yValue} and ${3:yValue}",
            "df_grouped = df.groupby('${1:xValue}')[['${2:yValue}', '${3:yValue}']].mean()",
            "df_grouped.plot(kind='bar')",
            "",
            "plt.title('Plot ${1:xValue} against ${2:yValue} and ${3:yValue}')",
            "plt.xlabel('${1:xValue}')",
            "plt.ylabel('${2:yValue} and ${3:yValue}')",
            "",
            "plt.legend(labels=['${2:yValue} and ${3:yValue}'])",
            "plt.xticks(df_grouped.index, [int(i) for i in df_grouped.index])",
            "plt.show()"
        ],
        "description": "Multi Bar Plot Matplot"
    },
    "Multi Line Plot Seaborn": {
        "prefix": "line_multi_sns",
        "body": [
            "#Plot ${1:xValue} against ${2:yValue} and ${3:yValue}",
            "sns.lineplot(data = df, x='${1:xValue}', y='${2:yValue}', label='${2:yValue}', errorbar=None)",
            "sns.lineplot(data = df, x='${1:xValue}', y='${3:yValue}', label='${3:yValue}', errorbar=None)",
            "",
            "plt.title('Plot ${1:xValue} against ${2:yValue} and ${3:yValue}')",
            "plt.xlabel('${1:xValue}')",
            "plt.ylabel('${2:yValue} and ${3:yValue}')",
            "",
            "plt.legend(labels=['${2:yValue} and ${3:yValue}'])",
            "plt.xticks(df_grouped.index, [int(i) for i in df_grouped.index])",
            "plt.show()"
        ],
        "description": "Multi Line Plot Seaborn"
    },
    "Scatter Plot Matplot": {
        "prefix": "scatter_plt",
        "body": [
            "#Plot ${1:xValue} against ${2:yValue}",
            "plt.scatter(df['${1:xValue}'], df['${2:xValue}'])",
            "",
            "plt.title('Plot ${1:xValue} against ${2:yValue}')",
            "plt.xlabel('${1:xValue}')",
            "plt.ylabel('${2:xValue}');",
            "",
            "plt.legend(labels=['${2:yValue}'])",
            "plt.show()"
        ],
        "description": "Scatter Plot Matplot"
    },
    "Decision Tree Plot Matplot": {
        "prefix": "decision_tree_plt",
        "body": [
            "#Plot decision tree",
            "fig = plt.figure(figsize=(25,20))",
            "plot = plot_tree(model, feature_names=['${1:target}'], filled=True)"
        ],
        "description": "Decision Tree Plot Matplot"
    },
    "Roc Curve Plot Matplot": {
        "prefix": "roc_curve_plt",
        "body": [
            "probs = model.predict_proba(X_test)[:, 1]",
            "",
            "# Calculate false positive rates and true positive rates",
            "base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])",
            "model_fpr, model_tpr, _ = roc_curve(y_test, probs)",
            "",
            "#Plot Roc Curve",
            "plt.figure(figsize = (8, 6))",
            "",
            "plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')",
            "plt.plot(model_fpr, model_tpr, 'r', label = 'model')",
            "plt.legend();",
            "plt.xlabel('False Positive Rate');",
            "plt.ylabel('True Positive Rate'); plt.title('ROC Curves');",
            "plt.show();"
        ],
        "description": "Roc Curve Plot Matplot"
    },
    // SPLIT
    "Split Train-Test": {
        "prefix": "split_train-test",
        "body": [
            "# Define features and target variable (${1:feature})",
            "X = df.drop('${1:target}', axis=1)",
            "y = df['${1:target}']",
            "",
            "# Split into train and test set",
            "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size=0.2, random_state=RSEED)"
        ],
        "description": "Split Train-Test"
    },
    "Split Train-Test Time Series": {
        "prefix": "split_train-test_timeserie",
        "body": [
            "# Define features and target variable (${1:feature})",
            "X = df.drop('${1:feature}', axis=1)",
            "y = df['${1:feature}']",
            "",
            "# Create TimeSeriesSplit object",
            "tscv = TimeSeriesSplit(n_splits=5)",
            "",
            "# Perform time series cross-validation",
            "for fold, (train_index, test_index) in enumerate(tscv.split(X), 1):",
            "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]",
            "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
        ],
        "description": "Split Train-Test Time Series"
    },
    // REGULARIZATION 
    "Data Augmentation": {
        "prefix": "data_augmentation",
        "body": [
            "# Data augmentation: Applies random distortions and transformations to the images",
            "augmentation = image.ImageDataGenerator(",
            "    preprocessing_function=preprocess_input,",
            "    fill_mode='nearest',",
            "    rotation_range=20,",
            "    width_shift_range=0.2,",
            "    height_shift_range=0.2,",
            "    horizontal_flip=True, ",
            "    zoom_range=0.2,",
            "    shear_range=0.2",
            ")"
        ],
        "description": "Data Augmentation"
    },
    // DECOMPOSING
    "Seasonal Decomposition": {
        "prefix": "decomposing_seasonal",
        "body": [
            "# Perform seasonal decomposition",
            "result = seasonal_decompose(df['target'], model='additive', period=12)",
            "",
            "# Extract components",
            "trend = result.trend",
            "seasonal = result.seasonal",
            "residual = result.resid",
            "",
            "# Create new features from the decomposition",
            "df['trend'] = trend",
            "df['seasonal'] = seasonal",
            "df['residual'] = residual",
            "",
            "# Fill NaN values that may result from the decomposition",
            "df = df.fillna(method='bfill').fillna(method='ffill')"
        ],
        "description": "Seasonal Decomposition"
    },
    // STEMMING
    "STEMMING": {
        "prefix": "stemming",
        "body": [
            "# Stem a corpus",
            "stemmer = nltk.PorterStemmer()",
            "",
            "# Create an analyzer with ${1:CountVectorizer/TfidfVectorizer}",
            "analyzer = ${1:CountVectorizer/TfidfVectorizer}().build_analyzer()",
            "",
            "def words_stemmed(doc):",
            "    # For each word in the document, apply the stemmer",
            "    return (stemmer.stem(w) for w in analyzer(doc))",
            "",
            "# Create a new ${1:CountVectorizer/TfidfVectorizer} vectorizer with the stemming function",
            "stemming = ${1:CountVectorizer/TfidfVectorizer}(analyzer=words_stemmed)",
            "",
            "# Apply the stemming vectorizer to the training data",
            "X_train = stemming.fit_transform(X_train)"
        ],
        "description": "STEMMING"
    },
    //LEMMATIZATION
    "Lemmatization": {
        "prefix": "lemmatization",
        "body": [
            "# Stem a corpus",
            "lemmer = nltk.WordNetLemmatizer()",
            "",
            "# Create an analyzer with ${1:CountVectorizer/TfidfVectorizer}",
            "analyzer = ${1:CountVectorizer/TfidfVectorizer}().build_analyzer()",
            "",
            "def words_lemmatized(doc):",
            "    # For each word in the document, apply the lemmer",
            "    return (lemmer.lemmatize(w) for w in analyzer(doc))",
            "",
            "# Create a new ${1:CountVectorizer/TfidfVectorizer} vectorizer with the lemmatizing function",
            "lemmatizing = ${1:CountVectorizer/TfidfVectorizer}(analyzer = words_lemmatized)",
            "",
            "# Apply the stemming vectorizer to the training data",
            "X_train = lemmatizing.fit_transform(X_train)"
        ],
        "description": "Lemmatization"
    },
    //ZERO-SHOT
    "Zero-Shot Classifier": {
        "prefix": "zero-shot_classifier",
        "body": [
            "# Initialize a zero-shot classification pipeline",
            "classifier = pipeline('zero-shot-classification')",
            "",
            "pred = classifier(sequence, candidate_labels, multi_label=True)",
            "",
            "def get_predictions_score(prediction):",
            "    pred_labels = prediction['labels']",
            "    pred_scores = prediction['scores']",
            "    seq = [prediction['sequence']]",
            "    return  pd.concat([",
            "                pd.DataFrame(seq),",
            "                pd.DataFrame(pred_labels),",
            "                pd.DataFrame(pred_scores),], axis=1, ignore_index=True).rename(columns={0:'Sequence', 1:'Labels', 2:'Probability'}).set_index(['Sequence'])",
            "get_predictions_score(pred)"
        ],
        "description": "Zero-Shot Classifier"
    },
    // REGRESSION
    "Linear Regression Model": {
        "prefix": "linear_regressor",
        "body": [
            "# Instantiate and train linear regression model",
            "model = LinearRegression()",
            "model.fit(X_train, y_train)"
        ],
        "description": "Linear Regression Model"
    },
    "Polynomial Feature Model": {
        "prefix": "polynomial_feature",
        "body": [
            "# Instantiate and train ${1:degree} degree polynomial model",
            "model = PolynomialFeatures(degree=${1:degree})",
            "model.fit_transform(X_train, y_train)"
        ],
        "description": "Linear Regression Model"
    },
    "Ridge Regression Model": {
        "prefix": "ridge_regressor",
        "body": [
            "# Instantiate and train Ridge regression model",
            "model = Ridge(alpha=1, random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Ridge Regression Model"
    },
    "Lasso Regression Model": {
        "prefix": "lasso_regressor",
        "body": [
            "# Instantiate and train Lasso regression model",
            "model = Lasso(alpha=0.5, max_iter=int(10e5), random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Lasso Regression Model"
    },
    "KNN Regression": {
        "prefix": "knn_regressor",
        "body": [
            "# Instantiate and train KNN Regression model",
            "model = KNeighborsRegressor()",
            "model.fit(X_train, y_train)"
        ],
        "description": "KNN Regression"
    },
    "Decision Tree Regressor": {
        "prefix": "decision_tree_regressor",
        "body": [
            "# Instantiate and train decision tree regressor on ${1:poisson/squared_error/absolute_error/friedman_mse}",
            "model = DecisionTreeRegressor(andom_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Decision Tree Regressor"
    },
    "Extreme Random Tree Regressor": {
        "prefix": "decision_tree_random_regressor",
        "body": [
            "# Instantiate and train extreme random tree regressor}",
            "model = ExtraTreesRegressor(random_state=RSEED, max_features='sqrt', n_jobs=-1, verbose=1)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Extreme Random Tree Regressor"
    },
    "Random Forest Regressor": {
        "prefix": "random_forest_regressor",
        "body": [
            "# Instantiate and train Random Forest Regressor model",
            "model = RandomForestRegressor(random_state=RSEED, max_features='sqrt', n_jobs=-1, verbose=1)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Random Forest Regressor"
    },
    "Neuronal Network Regressor": {
        "prefix": "fnn_regressor",
        "body": [
            "# Instantiate and train regression fnn model",
            "model = tf.keras.Sequential([scaler,",
            "                layers.Dense(units=1),",
            "                layers.Dense(64, activation='gelu', kernel_regularizer=regularizers.l2(0.01)),",
            "                layers.Dropout(0.25),",
            "                layers.Dense(64, activation='gelu', kernel_regularizer=regularizers.l2(0.01)),",
            "                layers.Dense(1)",
            "])",
            "model.compile(optimizer=tf.optimizers.legacy.${1:Adam/RMSProp}(learning_rate=0.1), loss='mae', metrics='mse')",
            "model.fit(X_train, y_train, epochs=100, verbose=0, callbacks=[callback])"
        ],
        "description": "Neuronal Network Regressor"
    },
    "CNN Regressor": {
        "prefix": "cnn_regressor",
        "body": [
            "# Instantiate and train CNN regressor model",
            "model = Sequential([",
            "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),",
            "    MaxPooling2D((2, 2)),",
            "",
            "    Conv2D(64, (3, 3), activation='relu'),",
            "    MaxPooling2D((2, 2)),",
            "",
            "    Conv2D(64, (3, 3), activation='relu'),",
            "",
            "    Flatten(),",
            "    Dense(64, activation='relu'),",
            "    Dense(32, activation='relu'),",
            "    Dense(1) ",
            "])",
            "",
            "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error' ])",
            "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
        ],
        "description": "CNN Regressor"
    },
    // Classification
    "SGD Classifier": {
        "prefix": "sgd_classifier",
        "body": [
            "# Instantiate and train SGDClassifier model",
            "model = SGDClassifier(random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "SGD Classifier"
    },
    "Logistic Regression Model": {
        "prefix": "logistic_regressor",
        "body": [
            "# Instantiate and train logistic regression model",
            "model = LogisticRegression(random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Logistic Regression Model"
    },
    "KNN Classifier": {
        "prefix": "knn_classifier",
        "body": [
            "# Instantiate and train KNN Classifier model",
            "model = KNeighborsClassifier()",
            "model.fit(X_train, y_train)"
        ],
        "description": "KNN Classifier"
    },
    "Decision Tree Classifier": {
        "prefix": "decision_tree_classifier",
        "body": [
            "# Instantiate and train decision tree classifier",
            "model = DecisionTreeClassifier(random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Decision Tree Classifier"
    },
    "Random Forest Classifier": {
        "prefix": "random_forestclassifier",
        "body": [
            "# Instantiate and train Random Forest Classifier model",
            "model = RandomForestClassifier(random_state=RSEED, max_features = 'sqrt', n_jobs=-1, verbose = 1)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Random Forest Classifier"
    },
    "Extreme Random Tree Classifier": {
        "prefix": "decision_tree_random_classifier",
        "body": [
            "# Instantiate and train extreme random tree regressor}",
            "model = ExtraTreesClassifier(random_state=RSEED, max_features='sqrt', n_jobs=-1, verbose=1)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Extreme Random Tree Classifier"
    },
    "XGB Classifier": {
        "prefix": "xgb_classifier",
        "body": [
            "# Instantiate and train XGBoost Classifier model",
            "model = XGBClassifier(random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "XGB Classifier"
    },
    "Support Vector Machines": {
        "prefix": "svc_classifier",
        "body": [
            "# Instantiate and train Support Vector Machines",
            "model = SVC(random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Support Vector Machines"
    },
    "Ada Boost Classifier": {
        "prefix": "ada_boost_classifier",
        "body": [
            "# Instantiate and train Ada Boost classifier",
            "model = AdaBoostClassifier(random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Ada Boost Classifier"
    },
    "Max Voting Classifier": {
        "prefix": "voting_classifier_max",
        "body": [
            "# Instantiate models",
            "model1 = LogisticRegression(random_state = RSEED)",
            "model2 = KNeighborsClassifier()",
            "model3 = DecisionTreeClassifier(random_state = RSEED)",
            "",
            "# Instantiate and train Voting Classifier model",
            "model = VotingClassifier(estimators=[('lr', model1), ('knn', model2), ('dt', model3)], voting='${1:hard/soft}')",
            "model.fit(X_train, y_train)",
            "model.score(X_test, y_test)"
        ],
        "description": "Max Voting Classifier"
    },
    "Gradient Boosting Classifier": {
        "prefix": "gradient_boosting_classifier",
        "body": [
            "# Hyperparameter grid",
            "estimators = [",
            "    ('dt', DecisionTreeClassifier(random_state=RSEED)),",
            "    ('knn', KNeighborsClassifier()),",
            "    ('rf', RandomForestClassifier(random_state=RSEED))",
            "    ]",
            "",
            "# Instantiate and train Stacking ${1:Classifier/Regressor} model",
            "model = Stacking${1:Classifier/Regressor}(estimators = estimators, final_estimator = LogisticRegression())",
            "model.fit(X_train, y_train).score(X_test, y_test)"
        ],
        "description": "Gradient Boosting Classifier"
    },
    "Neuronal Network Classifier": {
        "prefix": "fnn_classifier",
        "body": [
            "# Instantiate and train classifier fnn model",
            "model = tf.keras.Sequential([scaler,",
            "                layers.Dense(4, activation='gelu', kernel_regularizer=regularizers.l2(0.01)),",
            "                layers.Dropout(0.25),",
            "",
            "                layers.Dense(4, activation='gelu', kernel_regularizer=regularizers.l2(0.01)),",
            "                layers.Dropout(0.2),",
            "",
            "                layers.Dense(1, activation='sigmoid')",
            "])",
            "model.compile(optimizer=tf.optimizers.legacy.${1:Adam/RMSProp}(learning_rate=0.0005), loss=losses.BinaryCrossentropy(), metrics=[metrics.Precision(thresholds=0.5), metrics.Recall(thresholds=0.5)])",
            "model.fit(X_train, y_train, epochs=100, verbose=0, callbacks=[callback])"
        ],
        "description": "Neuronal Network Classifier"
    },
    "CNN Binary Classifier": {
        "prefix": "cnn_classifier_binary",
        "body": [
            "# Instantiate and train CNN Binary Classifier model",
            "model = Sequential([",
            "    Conv2D(filters=6, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(28, 28, 1)),",
            "    MaxPooling2D(pool_size=(2, 2), strides=2),",
            "",
            "    Conv2D(filters=16, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),",
            "    MaxPooling2D(pool_size=(2, 2), strides=2),",
            "",
            "    Flatten(),",
            "    Dense(units=64, activation='relu'),",
            "    Dense(units=1, activation='sigmoid')",
            "])",
            "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])",
            "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
        ],
        "description": "CNN Multiple Binary"
    },
    "CNN Multiple Classifier": {
        "prefix": "cnn_classifier_multiple",
        "body": [
            "# Instantiate and train CNN Multi-Class Classifier model",
            "model = Sequential([",
            "    Conv2D(filters=6,kernel_size=(3, 3), strides=(1, 1),padding='same', activation='relu', input_shape=(28, 28, 1)),",
            "    MaxPooling2D(pool_size=(2, 2), strides=2),",
            "",
            "    Conv2D(filters=16,kernel_size=(3, 3), strides=(1, 1),padding='same', activation='relu'),",
            "    MaxPooling2D(pool_size=(2, 2), strides=2),",
            "",
            "    Flatten(),",
            "    Dense(units=10, activation='softmax')",
            "])",
            "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])",
            "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
        ],
        "description": "CNN Multiple Classifier"
    },
    // SEARCH
    "Grid Search": {
        "prefix": "search_grid",
        "body": [
            "# Hyperparameter grid",
            "param_grid = {",
            "    'n_neighbors': [2, 4, 3, 5, 10],",
            "    'weights': ['uniform', 'distance'],",
            "    'p': [1, 2, 3],",
            "    'algorithm': ['ball_tree', 'kd_tree', 'brute']",
            "}",
            "",
            "# Instantiate and train ${1:model} model",
            "estimator = ${1:model}()",
            "",
            "# Instantiate gridsearch and define the metric to optimize",
            "model = GridSearchCV(scoring=scorer, estimator, param_grid, scoring='accuracy', cv=5, verbose=5, n_jobs=-1)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Grid Search"
    },
    "Random Search": {
        "prefix": "search_random",
        "body": [
            "# Hyperparameter grid",
            "param_grid = {",
            "    'n_estimators': np.arange(10, 201).astype(int),",
            "    'max_depth': [None] + list(np.arange(3, 21).astype(int)),",
            "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),",
            "    'max_leaf_nodes': [None] + list(np.arange(10, 51).astype(int)),",
            "    'min_samples_split': [2, 5, 10],",
            "    'bootstrap': [True, False]",
            "}",
            "",
            "# Instantiate and train ${1:modeltype} model",
            "estimator = ${1:modeltype}",
            "",
            "# Instantiate and train Random Search model",
            "model = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, scoring = 'roc_auc', cv = 3, n_iter = 10, verbose = 5, random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Random Search"
    },
    // ENSEMBLE
    "Averaging": {
        "prefix": "voting_averaging",
        "body": [
            "# Instantiate models",
            "model1 = LogisticRegression(random_state=RSEED)",
            "model2 = KNeighborsClassifier()",
            "model3 = DecisionTreeClassifier(random_state=RSEED)",
            "",
            "# Train models",
            "model1.fit(X_train,y_train)",
            "model2.fit(X_train,y_train)",
            "model3.fit(X_train,y_train)",
            "",
            "# Predict",
            "pred1 = model1.predict_proba(X_test)",
            "pred2 = model2.predict_proba(X_test)",
            "pred3 = model3.predict_proba(X_test)",
            "",
            "# Averaging",
            "finalpred = (pred1 + pred2 + pred3) / 3",
            "finalpred = np.argmax(finalpred.round(0), axis = 1)",
            "(y_test == finalpred).sum() / len(finalpred)"
        ],
        "description": "Averaging"
    },
    "Weighted Averaging": {
        "prefix": "voting_weighted_averaging",
        "body": [
            "# Instantiate models",
            "model1 = LogisticRegression(random_state=RSEED)",
            "model2 = KNeighborsClassifier()",
            "model3 = DecisionTreeClassifier(random_state=RSEED)",
            "",
            "# Train models",
            "model1.fit(X_train,y_train)",
            "model2.fit(X_train,y_train)",
            "model3.fit(X_train,y_train)",
            "",
            "# Predict",
            "pred1 = model1.predict_proba(X_test)",
            "pred2 = model2.predict_proba(X_test)",
            "pred3 = model3.predict_proba(X_test)",
            "",
            "# Calculate accuracy score",
            "acc1 = accuracy_score(y_test, model1.predict(X_test))",
            "acc2 = accuracy_score(y_test, model2.predict(X_test))",
            "acc3 = accuracy_score(y_test, model3.predict(X_test))",
            "",
            "acc_sum = acc1 + acc2 + acc3",
            "",
            "# Weighting",
            "weight1 = acc1/acc_sum",
            "weight2 = acc2/acc_sum",
            "weight3 = acc3/acc_sum",
            "",
            "# Final predict",
            "finalpred = (pred1 * weight1 + pred2 * weight2 + pred3 * weight3)",
            "finalpred = np.argmax(finalpred.round(0), axis = 1)",
            "(y_test == finalpred).sum() / len(finalpred)"
        ],
        "description": "Weighted Averaging"
    },
    "Bagging": {
        "prefix": "bagging_classifier",
        "body": [
            "# Instantiate and train Bagging Classifier with base model DecisionTreeClassifier",
            "model = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=RSEED)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Bagging"
    },
    "Stacking": {
        "prefix": "voting_stacking",
        "body": [
            "# Hyperparameter grid",
            "estimators = [",
            "    ('dt', DecisionTreeClassifier(random_state=RSEED)),",
            "    ('knn', KNeighborsClassifier()),",
            "    ('rf', RandomForestClassifier(random_state=RSEED))",
            "    ]",
            "",
            "# Instantiate and train Stacking ${1:Classifier/Regressor} model",
            "model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression(), cv=5, stack_method='predict_proba')",
            "model.fit(X_train, y_train).score(X_test, y_test)"
        ],
        "description": "Stacking"
    },
    // TIME SERIES
    "Auto ARIMA": {
        "prefix": "timeseries_arima",
        "body": [
            "# Fit an auto ARIMA model (for stationary data)",
            "model = pm.auto_arima(df['target'], start_p=1, start_q=1,",
            "            test='adf', ",
            "            max_p=3, max_q=3, m=12, ",
            "            start_P=0, seasonal=True,",
            "            d=None, D=1, trace=True,",
            "            error_action='ignore', ",
            "            suppres_warnings=True, ",
            "            stepwise=True)",
            "model.fit(X_train, y_train)"
        ],
        "description": "Auto ARIMA"
    },
    "Prophet": {
        "prefix": "timeseries_prophet",
        "body": [
            "# Fit a Prophet model",
            "model = Prophet(",
            "    growth='linear',",
            "    changepoint_range=0.9,",
            "    yearly_seasonality=20,",
            "    seasonality_mode='multiplicative',",
            "    changepoint_prior_scale=0.05)",
            "",
            "model.fit(df)",
            "future = m.make_future_dataframe(periods=365)",
            "forecast = m.predict(future)"
        ],
        "description": "Prophet"
    },
    // NLP
    "CountVectorizer": {
        "prefix": "vectorizer_count",
        "body": [
            "# Fit the CountVectorizer to training data",
            "vect = CountVectorizer().fit(X_train)",
            "",
            "# Transform the documents in the training data to a document-term matrix",
            "X_train_vectorized = vect.transform(X_train)"
        ],
        "description": "CountVectorizer"
    },
    "TfidfVectorizer": {
        "prefix": "vectorizer_tfidf",
        "body": [
            "# Fit the TfidfVectorizer to the training data",
            "# This means a word should have been used in at least 15 SMS",
            "vect = TfidfVectorizer(min_df=15).fit(X_train)",
            "",
            "# Transform the documents in the training data to a document-term matrix",
            "X_train_vectorized = vect.transform(X_train)"
        ],
        "description": "TfidfVectorizer"
    },
    // PRETRAINED MODEL
    "Pretrained Model": {
        "prefix": "model_pretrained",
        "body": [
            "# Select the pretrained ${1:model} model",
            "model = ${1:model}(include_top=False)",
            "",
            "# Freeze the weights",
            "model.trainable = False",
            "",
            "# Create model with pretrained network as base model",
            "inputs = Input(shape=(224, 224, 3))",
            "model = model(inputs)",
            "",
            "# Add additional cnn layers if necessary",
            "",
            "# Flatten out before final layer",
            "flatten = GlobalAveragePooling2D()(model)",
            "outputs = Dense(${2:number_of_classes}, activation='softmax')(flatten)",
            "model = Model(inputs, outputs)",
            "",
            "# Compile the model",
            "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])",
            "",
            "# Create batches for train",
            "classes = ['${3:class1}','${4:class2}']",
            "batch_train = data_gen.flow_from_directory(directory='data/train', class_mode='categorical', classes=classes, batch_size=150, target_size=(224, 224), )",
            "",
            "# Create batches for test",
            "batch_test = data_gen.flow_from_directory(directory='data/validation/', class_mode='categorical', classes=classes, batch_size=150, target_size=(224, 224), )",
            "",
            "# Train the model",
            "model_tf.fit(batch_train, verbose=2, epochs=10, validation_data=batch_test)"
        ],
        "description": "Pretrained Model"
    },
    // PREDICT
    "Predict": {
        "prefix": "predict",
        "body": [
            "# Predict",
            "y_pred_train = model.predict(X_train)",
            "y_pred_test = model.predict(X_test)"
        ],
        "description": "Predict"
    },
    "Predict Time Series": {
        "prefix": "predict_timeseries",
        "body": [
            "# Predict over ${1:numberofperiods} periods",
            "y_pred_train = model.predict(n_periods=${1:numberofperiods})",
            "y_pred_test = model.predict(n_periods=${1:numberofperiods})"
        ],
        "description": "Predict Time Series"
    },
    "Best Hyperparameter Select": {
        "prefix": "predict_best",
        "body": [
            "# Separate model with best hyperparameters",
            "model_best = model.best_estimator_",
            "",
            "# Make probability predictions",
            "prob_train = model_best.predict_proba(X_train)[: , 1]",
            "prob_test = model_best.predict_proba(X_test)[: , 1]",
            "",
            "# Predict Train and Test",
            "y_pred_train = model_best.predict(X_train)",
            "y_pred_test = model_best.predict(X_test)"
        ],
        "description": "Best Hyperparameter Select"
    },
    "Decode Predictions": {
        "prefix": "predict_decode",
        "body": [
            "# Decode labels",
            "pred = model.predict(img)",
            "decode_predictions(${1:pred})"
        ],
        "description": "Decode Predictions"
    },
    // METRICS
    "R-squared": {
        "prefix": "r_squared",
        "body": [
            "# Calculate r-squared",
            "r_squared_train = r2_score(y_train, y_pred_train)",
            "r_squared_test = r2_score(y_test, y_pred_test)",
            "print('R-squared (train): ', round(r_squared_train, 3))",
            "print('R-squared (test): ', round(r_squared_test,3))"
        ],
        "description": "R-squared"
    },
    "Adjusted R-squared": {
        "prefix": "r_squared_adjusted",
        "body": [
            "# Calculate adjusted r-squared",
            "r_squared_adjusted_train = 1 - ((1 - r_squared_train) * (X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1))",
            "r_squared_adjusted_test = 1 - ((1 - r_squared_test) * (X_train.shape[0] - 1) / (X_train.shape[0] - X_train.shape[1] - 1))",
            "print('R-squared adjusted (train): ', round(r_squared_adjusted_train,3))",
            "print('R-squared adjusted (test): ', round(r_squared_adjusted_test,3)"
        ],
        "description": "Adjusted R-squared"
    },
    "RMSE": {
        "prefix": "rmse",
        "body": [
            "# Calculate RMSE",
            "rmse_train = np.sqrt(mean_squared_error(y_train, y_pred_train))",
            "rmse_test = np.sqrt(mean_squared_error(y_test, y_pred_test))",
            "print('RMSE (train): ', round(rmse_train,3))",
            "print('RMSE (test): ', round(rmse_test,3))"
        ],
        "description": "R-RMSE"
    },
    "Confusion Matrix": {
        "prefix": "confusion_matrix",
        "body": [
            "# Calculate Confusion Matrix",
            "conf_matrix = confusion_matrix(y_test, y_pred)",
            "print('Confusion Matrix: ', conf_matrix)"
        ],
        "description": "Confusion Matrix"
    },
    "Accuracy Score": {
        "prefix": "accuracy_score",
        "body": [
            "# Calculate Accuracy",
            "accuracy_train = accuracy_score(y_test, y_pred_train)",
            "accuracy_test = accuracy_score(y_test, y_pred_test)",
            "print('Accuracy (train): ', round(accuracy_train, 2))",
            "print('Accuracy (test): ', round(accuracy_test, 2))"
        ],
        "description": "Accuracy Score"
    },
    "Recall Score": {
        "prefix": "recall_score",
        "body": [
            "# Calculate Recall",
            "recall_train = recall_score(y_test, y_pred_train, pos_label='2')",
            "recall_test = recall_score(y_test, y_pred_test, pos_label='2')",
            "print('Recall (train): ', round(recall_train, 2))",
            "print('Recall (test): ', round(recall_test, 2))"
        ],
        "description": "Recall Score"
    },
    "Precision Score": {
        "prefix": "precision_score",
        "body": [
            "# Calculate Precision",
            "precision_train = precision_score(y_test, y_pred_train)",
            "precision_test = precision_score(y_test, y_pred_test)",
            "print('Precision (train): ', round(precision_train, 2))",
            "print('Precision (test): ', round(precision_test, 2))"
        ],
        "description": "Precision Score"
    },
    "F1 Score": {
        "prefix": "f1_score",
        "body": [
            "# Calculate F1 Score",
            "f1_train = f1_score(y_test, y_pred_train)",
            "f1_test = f1_score(y_test, y_pred_test)",
            "print('F1 Score (train): ', round(f1_train,3))",
            "print('F1 Score (test): ', round(f1_test,3))"
        ],
        "description": "F1 Score"
    },
    "ROC Score": {
        "prefix": "roc_auc_score",
        "body": [
            "# Calculate ROC",
            "roc_auc_train = roc_auc_score(y_test, y_pred_train)",
            "roc_auc_test = roc_auc_score(y_test, y_pred_test)",
            "print('ROC (train): ', round(roc_auc_train, 2))",
            "print('ROC (test): ', round(roc_auc_test, 2))"
        ],
        "description": "ROC Score"
    },
    "Classification Report": {
        "prefix": "classification_report",
        "body": [
            "# Calculate Classification Report",
            "classification_train = classification_report(y_train, y_pred_train)",
            "classification_test = classification_report(y_test, y_pred_test)",
            "print('Classification Report (train): ', classification_train)",
            "print('Classification Report (test): ', classification_test)"
        ],
        "description": "Classification Report"
    },
    "Validation Curve": {
        "prefix": "validation_curve",
        "body": [
            "# Assign trained model to variable",
            "history = model.fit(${1:parameters})",
            "",
            "# Plotting function for ${2:mse/loss}",
            "plt.plot(history.history['${2:mse/loss/accurency}'])",
            "plt.plot(history.history['${3:val_mse/val_loss/val_acuracy}'])",
            "plt.title('Model ${2:mse/loss}')",
            "plt.ylabel('${2:mse/loss}')",
            "plt.xlabel('Epoch')",
            "plt.legend(['train', 'validation'], loc='upper right')",
            "plt.show()"
        ],
        "description": "Validation Curve"
    },
    "Augmented Dickey-Fuller": {
        "prefix": "adf_test",
        "body": [
            "# Perform the ADF test",
            "adf_result = adfuller(time_series)",
            "",
            "# Extract and display the results",
            "adf_statistic = adf_result[0]",
            "p_value = adf_result[1]",
            "critical_values = adf_result[4]",
            "",
            "print('ADF Statistic:', adf_statistic)",
            "print('p-value:', p_value)",
            "print('Critical Values:')",
            "for key, value in critical_values.items():",
            "    print(f'   {key}: {value}')"
        ],
        "description": "Augmented Dickey-Fuller"
    },
    "Evaluate Modul Function": {
        "prefix": "def_evaluate_model",
        "body": [
            "# Make probability predictions",
            "prob_train = model.predict_proba(X_train)[: , 1]",
            "prob_test = model.predict_proba(X_test)[: , 1]",
            "",
            "# Predict Train and Test",
            "y_pred_train = model.predict(X_train)",
            "y_pred_test = model.predict(X_test)",
            "",
            "def evaluate_model(y_pred_test, prob_test, y_pred_train, prob_train):",
            "    '''Compare machine learning model to baseline performance.",
            "    Computes statistics and shows ROC curve.'''",
            "",
            "    baseline = {}",
            "",
            "    baseline['recall'] = recall_score(y_test, [1 for _ in range(len(y_test))])",
            "    baseline['precision'] = precision_score(y_test, [1 for _ in range(len(y_test))])",
            "    baseline['roc'] = 0.5",
            "    results = {}",
            "",
            "    results['recall'] = recall_score(y_test, y_pred_test)",
            "    results['precision'] = precision_score(y_test, y_pred_test)",
            "    results['roc'] = roc_auc_score(y_test, prob_test)",
            "",
            "    train_results = {}",
            "    train_results['recall'] = recall_score(y_train, y_pred_train)",
            "    train_results['precision'] = precision_score(y_train, y_pred_train)",
            "    train_results['roc'] = roc_auc_score(y_train, prob_train)",
            "",
            "    for metric in ['recall', 'precision', 'roc']:",
            "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')",
            "",
            "    # Calculate false positive rates and true positive rates",
            "    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])",
            "    model_fpr, model_tpr, _ = roc_curve(y_test, prob_test)",
            "",
            "    plt.figure(figsize = (8, 6))",
            "    plt.rcParams['font.size'] = 16",
            "",
            "    # Plot both curves",
            "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')",
            "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')",
            "    plt.legend();",
            "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');",
            "",
            "evaluate_model(y_pred_test, prob_test, y_pred_train, prob_train)"
        ],
        "description": "Evaluate Modul Function"
    },
    // MAIN
    "Calling Main": {
        "prefix": "main",
        "body": [
            "# Calling main function",
            "if __name__ == '__main__':",
            "    main()"
        ],
        "description": "Calling Main"
    }
}